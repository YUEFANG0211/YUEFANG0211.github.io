---
title: "Sarah: Hallucination Detection for Large Vision Language Models with Semantic Information Locator and Purifier in Uncertainty Quantification Method"
collection: publications
category: manuscripts
permalink: /publication/2025-10-01-paper-title-number-1
excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2025-10-01
venue: 'IMAVIS'
slidesurl: 'http://yuefang0211.github.io/files/sarah_slide1.pdf'
paperurl: 'http://yuefang0211.github.io/files/sarah1.pdf'
bibtexurl: 'http://yuefang0211.github.io/files/bibtex1.bib'
citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
Large Vision-Language Models (LVLMs) have demonstrated remarkable potential in multi-modal applications, yet their reliability is compromised by hallucination -- misalignment between generated text and visual inputs, linguistic context, or factual knowledge. To address this urgent challenge, we propose Sarah (Hallucination Detection for Large Vision Language Models with Semantic Information Locator and Purifier in Uncertainty Quantification Method), a novel hallucination detection framework grounded in uncertainty quantification method. Different from most existing uncertainty-based methods that utilize the variance of multi-round inference and need complex external tools, Sarah requires only single-round of inference result and minimal dependence on external tools, delving deeply into the value of the probability distribution. Considering uneven distribution of semantic information in both complete generation as well as possible outputs per-step, Semantic Information Locator and Purifier are proposed to enhance semantic collaboration and reduce semantic interference. Our extensive experiments across 5 off-the-shelf LVLMs and 2 open-ended visual question-answering benchmarks demonstrate that Sarah demonstrates superior performance by outperforming five out of six selected strong baseline methods in hallucination detection, while achieving comparable detection accuracy to the remaining one with significantly enhanced cost-effectiveness (requires only 1/25 of the computational time per iteration). Specifically, on image captioning outputs generated by GPT-4o, Sarah achieves a hallucination detection accuracy of 86.6%. Analysis over LVLMs further exposes critical limitations: over 13.4% of outputs from state-of-the-art LVLMs exhibit hallucination, which leaves room for future improvements. 

<!-- ===== 多媒体展示：视频（含题注） ===== -->
<figure style="margin: 1.5rem 0;">
  <video controls preload="metadata" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.12);">
    <source src="/assets/videos/sarah_demo.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption style="text-align: center; color: #666; font-size: 0.95rem; margin-top: 0.5rem;">
    图 1：Sarah 系统演示视频（示例占位，替换为实际文件路径与说明）
  </figcaption>
  
</figure>

<!-- 正文占位（可替换为实际内容） -->

这里是一段正文内容（待定）。可在此处补充视频与图像之间的说明性文字，例如：对实验设置、可视化含义或用户交互流程的简要描述。

<!-- ===== 多媒体展示：图片（含题注） ===== -->
<figure style="margin: 1.5rem 0;">
  <img src="/assets/images/sarah_overview.png" alt="Sarah framework overview" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.12);" />
  <figcaption style="text-align: center; color: #666; font-size: 0.95rem; margin-top: 0.5rem;">
    图 2：Sarah 框架概览示意图（示例占位，替换为实际图片与题注）
  </figcaption>
</figure>
