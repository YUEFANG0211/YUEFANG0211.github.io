---
title: "Sarah: Hallucination Detection for Large Vision Language Models with Semantic Information Locator and Purifier in Uncertainty Quantification Method"
collection: publications
category: manuscripts
permalink: /publication/2025-10-01-paper-title-number-1
excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2025-10-01
venue: 'IMAVIS'
slidesurl: 'http://yuefang0211.github.io/files/sarah_slide1.pdf'
paperurl: 'http://yuefang0211.github.io/files/sarah1.pdf'
bibtexurl: 'http://yuefang0211.github.io/files/bibtex1.bib'
citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
Large Vision-Language Models (LVLMs) have demonstrated remarkable potential in multi-modal applications, yet their reliability is compromised by hallucination -- misalignment between generated text and visual inputs, linguistic context, or factual knowledge. To address this urgent challenge, we propose Sarah (Hallucination Detection for Large Vision Language Models with Semantic Information Locator and Purifier in Uncertainty Quantification Method), a novel hallucination detection framework grounded in uncertainty quantification method. Different from most existing uncertainty-based methods that utilize the variance of multi-round inference and need complex external tools, Sarah requires only single-round of inference result and minimal dependence on external tools, delving deeply into the value of the probability distribution. Considering uneven distribution of semantic information in both complete generation as well as possible outputs per-step, Semantic Information Locator and Purifier are proposed to enhance semantic collaboration and reduce semantic interference. Our extensive experiments across 5 off-the-shelf LVLMs and 2 open-ended visual question-answering benchmarks demonstrate that Sarah demonstrates superior performance by outperforming five out of six selected strong baseline methods in hallucination detection, while achieving comparable detection accuracy to the remaining one with significantly enhanced cost-effectiveness (requires only 1/25 of the computational time per iteration). Specifically, on image captioning outputs generated by GPT-4o, Sarah achieves a hallucination detection accuracy of 86.6\%. Analysis over LVLMs further exposes critical limitations: over 13.4\% of outputs from state-of-the-art LVLMs exhibit hallucination, which leaves room for future improvements. 
