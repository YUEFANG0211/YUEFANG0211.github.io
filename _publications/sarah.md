---
title: "Sarah: Hallucination Detection for Large Vision Language Models with Semantic Information Locator and Purifier in Uncertainty Quantification Method"
collection: publications
category: manuscripts
permalink: /publication/sarah
excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2025-10-01
venue: 'IMAVIS'
slidesurl: 'http://yuefang0211.github.io/files/sarah_slide1.pdf'
paperurl: 'http://yuefang0211.github.io/files/sarah1.pdf'
bibtexurl: 'http://yuefang0211.github.io/files/bibtex1.bib'
citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
Sarah (Hallucination Detection for Large Vision Language Models with Semantic Information Locator and Purifier in Uncertainty Quantification Method) is a novel hallucination detection framework grounded in uncertainty quantification method. 

<!-- ===== 多媒体展示：视频（含题注） ===== -->
<figure style="margin: 1.5rem 0;">
  <video controls preload="metadata" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.12);">
    <source src="/images/sarah_demo.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption style="text-align: center; color: #666; font-size: 0.95rem; margin-top: 0.5rem;">
    Fig1. Sarah Running on the Web
  </figcaption>
</figure>

<!-- 正文占位（可替换为实际内容） -->
Different from most existing uncertainty-based methods that utilize the variance of multi-round inference and need complex external tools, Sarah requires only single-round of inference result and minimal dependence on external tools, delving deeply into the value of the probability distribution. Considering uneven distribution of semantic information in both complete generation as well as possible outputs per-step, Semantic Information Locator and Purifier are proposed to enhance semantic collaboration and reduce semantic interference. 

<!-- ===== 多媒体展示：图片（含题注） ===== -->
<figure style="margin: 1.5rem 0;">
  <img src="/images/总框架.png" alt="Sarah framework overview" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.12);" />
  <figcaption style="text-align: center; color: #666; font-size: 0.95rem; margin-top: 0.5rem;">
    Fig2. Overview of the Sarah framework for hallucination detection in LVLMs.
  </figcaption>
</figure>

<!-- 正文占位（可替换为实际内容） -->

Our extensive experiments across 5 off-the-shelf LVLMs and 2 open-ended visual question-answering benchmarks demonstrate that Sarah demonstrates superior performance by outperforming five out of six selected strong baseline methods in hallucination detection, achieving comparable detection accuracy.Specifically, on image captioning outputs generated by GPT-4o, Sarah achieves a hallucination detection accuracy of 86.6%.

<!-- ===== 多媒体展示：图片（含题注） ===== -->
<figure style="margin: 1.5rem 0;">
  <img src="/images/检测结果.png" alt="Detection Performance" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.12);" />
  <figcaption style="text-align: center; color: #666; font-size: 0.95rem; margin-top: 0.5rem;">
    Fig.3 Comparison with state-of-the-arts on free-form benchmark (MSCOCO-Cap and Bingo) for LVLMs hallucination detection. 
  </figcaption>
</figure>

<!-- 正文占位（可替换为实际内容） -->

It has also  significantly enhanced cost-effectiveness (requires only 1/25 of the computational time per iteration).  

<!-- ===== 多媒体展示：图片（含题注） ===== -->
<figure style="margin: 1.5rem 0;">
  <img src="/images/耗时对比.png" alt="Time Consume" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.12);" />
  <figcaption style="text-align: center; color: #666; font-size: 0.95rem; margin-top: 0.5rem;">
    Fig4. Resource consumption across diverse hallucination detection methods. This analysis evaluates whether a detection method requires (1) multi-round reasoning and (2) image inputs, both of which significantly affect GPU memory requirements. The actual iteration time for each method is also provided in the table.
  </figcaption>
</figure>

<!-- 正文占位（可替换为实际内容） -->

Analysis over LVLMs further exposes critical limitations: over 13.4% of outputs from state-of-the-art LVLMs exhibit hallucination, which leaves room for future improvements.

<!-- ===== 多媒体展示：图片（含题注） ===== -->
<figure style="margin: 1.5rem 0;">
  <img src="/images/模型对比.png" alt="Time Consume" style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.12);" />
  <figcaption style="text-align: center; color: #666; font-size: 0.95rem; margin-top: 0.5rem;">
    Fig5. Hallucination rate of different LVLMs based on Sarah.
  </figcaption>
</figure>
